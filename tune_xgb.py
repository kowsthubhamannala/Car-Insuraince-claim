import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from preprocess import build_preprocessor
from xgboost import XGBClassifier

def tune_xgb():
 print("ðŸ”Ž Running hyperparameter tuning (this may take some time)...")

 train = pd.read_csv("data/train.csv")
 X = train.drop(columns=["is_claim"])
 y = train["is_claim"]

 X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

 preprocessor = build_preprocessor(X_train)
 smote = SMOTE(sampling_strategy=0.3, random_state=42)

 xgb = XGBClassifier(eval_metric="logloss", use_label_encoder=False, random_state=42)

 pipe = ImbPipeline(steps=[
 ("preprocessor", preprocessor),
 ("smote", smote),
 ("clf", xgb)
 ])

 param_grid = {
 "clf__n_estimators": [200, 400],
 "clf__learning_rate": [0.05, 0.1],
 "clf__max_depth": [3, 6],
 "clf__subsample": [0.6, 0.8],
 "clf__colsample_bytree": [0.6, 0.8],
 "clf__scale_pos_weight": [12, 16],
 }

 grid = GridSearchCV(pipe, param_grid, cv=3, scoring="roc_auc", verbose=2, n_jobs=-1)
 grid.fit(X_train, y_train)

 print("Best ROC-AUC score from CV:", grid.best_score_)
 print("Best parameters:", grid.best_params_)

if __name__ == "__main__":
 tune_xgb()

